{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f239612-620e-4430-8685-9fdc6b179b41",
   "metadata": {
    "id": "5f239612-620e-4430-8685-9fdc6b179b41"
   },
   "source": [
    "# Training PEFT models with new tokens being added to the embedding layers and tokenizer\n",
    "\n",
    "In this example, we will learn how to train a LoRA model when adding new tokens to the tokenizer and model.\n",
    "This is a common usecase when doing the following:\n",
    "1. Instruction finetuning with new tokens beind added such as `<|user|>`, `<|assistant|>`, `<|system|>`, `</s>`, `<s>` to properly format the conversations\n",
    "2. Finetuning on a specific language wherein language spoecific tokens are added, e.g., korean tokens being added to vocabulary for finetuning LLM on Korean datasets.\n",
    "3. Instruction finetuning to return outputs in certain format to enable agent behaviour new tokens such as `<|FUNCTIONS|>`, `<|BROWSE|>`, `<|TEXT2IMAGE|>`, `<|ASR|>`, `<|TTS|>`, `<|GENERATECODE|>`, `<|RAG|>`.\n",
    "\n",
    "In such cases, you add the Embedding modules to the LORA `target_modules`. PEFT will take care of saving the embedding layers with the new added tokens along with the adapter weights that were trained on the specific initialization of the embeddings weights of the added tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072e7c9-c1a5-4d61-a8ea-c5751769c430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b27c55e8-edaa-4059-90bc-d6096d596902",
   "metadata": {
    "id": "b27c55e8-edaa-4059-90bc-d6096d596902"
   },
   "source": [
    "Let's import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d413556b-4a3c-4ede-8642-ea5e772d83a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install aqlm[gpu]>=1.1.0\n",
    "!pip install git+https://github.com/huggingface/peft.git@main\n",
    "!pip install accelerate>=0.27.0\n",
    "!pip install git+https://github.com/huggingface/transformers.git@main\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes # for 8-bit optimizer only\n",
    "!pip install torch --upgrade\n",
    "!pip install huggingface --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0d78b-d846-4055-82b7-1232a0b64ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dataclass_csv\n",
    "!pip uninstall aqlm -y\n",
    "!pip install aqlm[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f864c90",
   "metadata": {
    "id": "6f864c90"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"PeftExamples\"\n",
    "import transformers\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    # prepare_model_for_int8_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74950a3f-bb63-4ce5-9e2b-1b83f92b13a2",
   "metadata": {
    "id": "74950a3f-bb63-4ce5-9e2b-1b83f92b13a2"
   },
   "source": [
    "## Prepare Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76763f5e-64b2-409b-8845-ae5589f8a4e0",
   "metadata": {
    "id": "76763f5e-64b2-409b-8845-ae5589f8a4e0"
   },
   "source": [
    "Now, we will be adding 27 new tokens as well as replace the existing pad, bos and eos tokens of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0498ea-547e-418d-bf13-c9abafdd5476",
   "metadata": {
    "id": "fd0498ea-547e-418d-bf13-c9abafdd5476"
   },
   "outputs": [],
   "source": [
    "class SpecialTokens(str, Enum):\n",
    "    begin_target = \"<|begintarget|>\"\n",
    "    end_target = \"<|endtarget|>\"\n",
    "    begin_context = \"<|begincontext|>\"\n",
    "    end_context = \"<|endcontext|>\"\n",
    "    system = \"<|system|>\"\n",
    "    user = \"<|user|>\"\n",
    "    begin_last_user_utterance = \"<|beginlastuserutterance|>\"\n",
    "    end_last_user_utterance = \"<|endlastuserutterance|>\"\n",
    "    begin_dsts = \"<|begindsts|>\"\n",
    "    end_dsts = \"<|enddsts|>\"\n",
    "    begin_dst = \"<|begindst|>\"\n",
    "    end_dst = \"<|enddst|>\"\n",
    "    begin_belief = \"<|beginbelief|>\"\n",
    "    end_belief = \"<|endbelief|>\"\n",
    "    begin_response = \"<|beginresponse|>\"\n",
    "    end_response = \"<|endresponse|>\"\n",
    "    begin_action = \"<|beginaction|>\"\n",
    "    end_action = \"<|endaction|>\"\n",
    "    begin_user_action = \"<|beginuseraction|>\"\n",
    "    end_user_action = \"<|enduseraction|>\"\n",
    "    sys_actions = \"<|sysactions|>\"\n",
    "    begin_intent = \"<|beginintent|>\"\n",
    "    end_intent = \"<|endintent|>\"\n",
    "    begin_requested_slots = \"<|beginrequestedslots|>\"\n",
    "    end_requested_slots = \"<|endrequestedslots|>\"\n",
    "    pad_token = \"<|pad|>\"\n",
    "    bos_token = \"<|startoftext|>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a4255-5f13-4eef-a024-4f1de0f2173b",
   "metadata": {
    "id": "ae4a4255-5f13-4eef-a024-4f1de0f2173b"
   },
   "source": [
    "We will be finetuning Mistral-7B model. Let's load the tokenizer and add the special tokens followed by loading the base model and resizzing the embedding layers to accomodate the newly added tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0eedef9",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "91c67b6377fc4dd7977bf544de784d51"
     ]
    },
    "id": "f0eedef9",
    "outputId": "6306ccce-1661-4b1b-baa8-635013200c6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128256, 4096)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name = \"ISTA-DASLab/Meta-Llama-3-8B-Instruct-AQLM-2Bit-1x16\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name,\n",
    "#     pad_token=SpecialTokens.pad_token.value,\n",
    "#     bos_token=SpecialTokens.bos_token.value,\n",
    "#     eos_token=SpecialTokens.end_target.value,\n",
    "#     additional_special_tokens=SpecialTokens.list(),\n",
    "#     token=\"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\"\n",
    "# )\n",
    "\n",
    "\n",
    "tokenizer_iqlm = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=\"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\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    token=\"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\"\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer_iqlm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88439ed6-9974-4918-80df-ec78b05b4185",
   "metadata": {
    "id": "88439ed6-9974-4918-80df-ec78b05b4185"
   },
   "source": [
    "## Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80967087",
   "metadata": {
    "id": "80967087",
    "outputId": "23df1152-5ada-4641-f4a9-73367a237208",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 44,204,032 || all params: 2,086,375,424 || trainable%: 2.1187\n",
      "None\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): lora.Embedding(\n",
      "          (base_layer): Embedding(128256, 4096)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict()\n",
      "          (lora_B): ModuleDict()\n",
      "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 64x128256 (cuda:0)])\n",
      "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 4096x64 (cuda:0)])\n",
      "          (lora_magnitude_vector): ModuleDict()\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.AqlmLoraLinear(\n",
      "                (base_layer): QuantizedLinear()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): QuantizedLinear()\n",
      "              (v_proj): lora.AqlmLoraLinear(\n",
      "                (base_layer): QuantizedLinear()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): QuantizedLinear()\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): QuantizedLinear()\n",
      "              (up_proj): QuantizedLinear()\n",
      "              (down_proj): QuantizedLinear()\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Identity()\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=64, out_features=128256, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=64, lora_alpha=128, lora_dropout=0.0, target_modules=[\"embed_tokens\", \"lm_head\", \"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print(model.print_trainable_parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac9945-4fcb-45f4-9478-d99a25a519cc",
   "metadata": {
    "id": "15ac9945-4fcb-45f4-9478-d99a25a519cc"
   },
   "source": [
    "## Preapre Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6980d59-42d4-4a27-84cc-a9719302088b",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "33d9539232da48f3ae922216b98ae462",
      "b7a33811d93742099140240cad91b679"
     ]
    },
    "id": "c6980d59-42d4-4a27-84cc-a9719302088b",
    "outputId": "0a0bcd05-da9c-4a98-f1e8-13585feeea42"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2395e4fad0d4455e9786ec32f7edbe71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2471d387482d49548faf6e7e754d4a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"smangrul/assistant_chatbot_dataset\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.2)\n",
    "\n",
    "text_column = \"context\"\n",
    "label_column = \"target\"\n",
    "max_length = 512\n",
    "tokenizer = tokenizer_iqlm\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    \n",
    "    # response of the assistant\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    \n",
    "    # tokenize the user query\n",
    "    model_inputs = tokenizer(examples[text_column])\n",
    "    print(model_inputs.keys())\n",
    "    \n",
    "    # tokenize the ansewr of the assistant\n",
    "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
    "    print(labels.keys())\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "\n",
    "        # get input and answers\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        \n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        # concatenate the input with the answer\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "\n",
    "        # for labels we mask out the inputs and concatenate with the answer\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        \n",
    "        # attention is set to 1 for all input \n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    \n",
    "\n",
    "\n",
    "    # work on padding all seqs\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        # get input and answers\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        \n",
    "        \n",
    "        # padd the whole input with pad_id from the left of the seq\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "\n",
    "\n",
    "        # apply the padding to the attention mask also\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "\n",
    "        \n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = model_inputs[\"input_ids\"][i][:max_length]\n",
    "        model_inputs[\"attention_mask\"][i] = model_inputs[\"attention_mask\"][i][:max_length]\n",
    "        labels[\"input_ids\"][i] = labels[\"input_ids\"][i][:max_length]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f38888e-4382-415b-869d-7202a816606a",
   "metadata": {
    "id": "3f38888e-4382-415b-869d-7202a816606a"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuf8ufle=True, collate_fn=default_data_collator, batch_size=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b9e552-4c5d-43e8-a9cd-8073af8d4280",
   "metadata": {
    "id": "53b9e552-4c5d-43e8-a9cd-8073af8d4280",
    "outputId": "047172a2-bebe-4e12-8fd0-d223f3198599"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         ...,\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257]]),\n",
       " 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         ...,\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de31ee2-185e-4658-9ad1-ae5f6bc3a611",
   "metadata": {
    "id": "7de31ee2-185e-4658-9ad1-ae5f6bc3a611",
    "outputId": "7606e5bb-a86a-41cb-b896-eefb2c565163"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|begincontext|><|user|>Hey, can you help me find a place to eat?<|system|>Sure, where abouts?<|user|>I want to eat in SF.<|system|>What type of food do you fancy? Chinese, Indian?<|user|>I'm feeling some Szcheuan.<|system|>Okay, I've found 10 restaurants that could be good. How does Alice's in San Francisco sound?<|user|>Sounds good. What's their number and address?<|system|>Their number is 415-282-8999. The address is 1599 Sanchez Street.<|user|>That's great, thanks.<|system|>Would you like me to make a reservation for you?<|user|>Yes, please. On the 1st of this month.<|system|>And for what time?<|user|>For around 11:45.<|system|>Sure thing, please confirm: reservation for 2 people at Alice's in San Francisco at 11:45 am today.<|beginlastuserutterance|>Yes that's good. Are they expensive? Is there live music?<|endlastuserutterance|><|endcontext|><|begintarget|><|begindsts|><|begindst|><|beginintent|>ReserveRestaurant<|endintent|><|beginrequestedslots|>Restaurants^has_live_music|Restaurants^price_range<|endrequestedslots|><|beginbelief|>Restaurants^city->SF~San Francisco|Restaurants^cuisine->Chinese~Szcheuan|Restaurants^date->1st of this month~today|Restaurants^party_size->2|Restaurants^restaurant_name->Alice's|Restaurants^time->11:45~11:45 am<|endbelief|><|enddst|><|enddsts|><|beginuseraction|>REQUEST->Restaurants^price_range~|REQUEST->Restaurants^has_live_music~|AFFIRM->Restaurants^~<|enduseraction|><|beginaction|>INFORM->Restaurants^price_range~inexpensive|INFORM->Restaurants^has_live_music~False|OFFER->Restaurants^restaurant_name~Alice's|OFFER->Restaurants^party_size~2|OFFER->Restaurants^date~today|OFFER->Restaurants^time~12 pm|NOTIFY_FAILURE->Restaurants^~<|endaction|><|beginresponse|>I'm sorry, I was unable to make a reservation at that time. Shall I try for a reservation at Alice's for 2 people today at 12 pm? Their pricing is inexpensive and they do not have live music.<|endresponse|><|endtarget|><|endtarget|>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0358e3-0e22-4a91-be62-c396b04cf332",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031a7ca4-c157-4de8-b559-e04e0012a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156526/156526 [00:05<00:00, 27010.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Create a nested loop to print every combination of numbers between 0-9, excluding any combination that contains the number 5. Additionally, exclude any combination that contains a repeating digit. Implement the solution without using any built-in functions or libraries to check for repeating digits.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"{'code': '\\\\nfor i in range(10):  # First digit\\\\n    for j in range(10):  # Second digit\\\\n        for k in range(10):  # Third digit\\\\n            # Checking for the conditions\\\\n            if i != 5 and j != 5 and k != 5 and i != j and i != k and j != k:\\\\n                print(i, j, k)\\\\n'}\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset(\"m-a-p/CodeFeedback-Filtered-Instruction\")\n",
    "\n",
    "def get_code(s):\n",
    "    s = s.split(\"```\")\n",
    "    for p in s:\n",
    "        if p.startswith('python'):\n",
    "            return p[6:]\n",
    "\n",
    "from tqdm import tqdm\n",
    "new_data = []\n",
    "for x in tqdm(raw_data['train']):\n",
    "\n",
    "    curr_chat = []\n",
    "    code = get_code(x['answer'])\n",
    "    if code is None or code==\"None\":\n",
    "        continue\n",
    "\n",
    "    chat = [\n",
    "    {\"role\": \"user\", \"content\": x[\"query\"]},\n",
    "    {\"role\": \"assistant\", \"content\": str({\"code\": code})}]\n",
    "    \n",
    "    new_data.append(chat)\n",
    "\n",
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43910e8d-0f85-4e80-b009-50ae8c05540a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23e1c64f89142759acadd005f33c531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenizer_iqlm.pad_token = tokenizer_iqlm.eos_token\n",
    "code_dataset = Dataset.from_dict({\"chat\": new_data[:100]})\n",
    "\n",
    "code_dataset = code_dataset.map(lambda x: {\"chat_format\": tokenizer_iqlm.apply_chat_template(x[\"chat\"], tokenize=False)}, \n",
    "                                    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e66cc06-551e-4c5c-ba4e-c99e31f6511b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCreate a nested loop to print every combination of numbers between 0-9, excluding any combination that contains the number 5. Additionally, exclude any combination that contains a repeating digit. Implement the solution without using any built-in functions or libraries to check for repeating digits.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{'code': '\\\\nfor i in range(10):  # First digit\\\\n    for j in range(10):  # Second digit\\\\n        for k in range(10):  # Third digit\\\\n            # Checking for the conditions\\\\n            if i != 5 and j != 5 and k != 5 and i != j and i != k and j != k:\\\\n                print(i, j, k)\\\\n'}<|eot_id|>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(code_dataset['chat_format'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5aaaa21-ae0d-4c98-babb-daf42ca9d2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e2e58fc7534865b16e14a9f3d87e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b1bfeaaa3043d48f29377327774f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "text_column = \"chat_format\"\n",
    "max_length = 512\n",
    "tokenizer = tokenizer_iqlm\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # tokenize the user query\n",
    "    model_inputs = tokenizer(examples[text_column])\n",
    "    print(model_inputs.keys())\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "\n",
    "        # get input and answers\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        \n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        # concatenate the input with the answer\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids \n",
    "\n",
    "        # attention is set to 1 for all input \n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    \n",
    "\n",
    "\n",
    "    # work on padding all seqs\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        # get input and answers\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # padd the whole input with pad_id from the left of the seq\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids))\n",
    "\n",
    "\n",
    "        # apply the padding to the attention mask also\n",
    "        model_inputs[\"attention_mask\"][i] = model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i] + [0] * (max_length - len(sample_input_ids)) \n",
    "\n",
    "        \n",
    "        \n",
    "        model_inputs[\"input_ids\"][i] = model_inputs[\"input_ids\"][i][:max_length]\n",
    "        model_inputs[\"attention_mask\"][i] = model_inputs[\"attention_mask\"][i][:max_length]\n",
    "        \n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "code_dataset_split = code_dataset.train_test_split(0.2)\n",
    "\n",
    "processed_datasets = code_dataset_split.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=code_dataset_split['train'].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "128952be-8c35-4322-a1a9-453472d0ed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a function to find the maximum difference between two prime numbers in a given array. The array can contain positive and negative integers, and can be unsorted. Additionally, the function should handle arrays of any length. The function should return the maximum difference as an absolute value. For example, for the array [5, 3, 17, 11, 9], the function should return 14.\\n\\nHowever, your function should have a time complexity of O(n), where n is the length of the array. Additionally, you should not use any built-in functions or libraries to check if a number is prime. You need to implement your own prime checking function.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset_split[\"test\"][0]['chat'][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7ec081a-2848-464c-942b-5a8c0cb598a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nDesign a data structure that efficiently stores an array of words, where each word is stored in a Trie data structure. The data structure should support the following operations:\\n\\n1. `insert(word)`: Inserts a new word into the data structure. The time complexity of this operation should be O(N), where N is the length of the word.\\n2. `search(word)`: Returns true if the word exists in the data structure, false otherwise. The time complexity of this operation should be O(N), where N is the length of the word.\\n3. `startsWith(prefix)`: Returns a list of all words in the data structure that have the given prefix. The time complexity of this operation should be O(K), where K is the number of words with the given prefix.\\n\\nAdditionally, the space complexity of the data structure should be O(M), where M is the total number of characters in all the words in the data structure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{'code': '\\\\nclass TrieNode:\\\\n    def __init__(self):\\\\n        self.children = {}\\\\n        self.is_end_of_word = False\\\\n'}<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[20][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d1c83-196d-471e-9bf7-5f36dafa9894",
   "metadata": {
    "id": "239d1c83-196d-471e-9bf7-5f36dafa9894"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec80d6ee",
   "metadata": {
    "id": "ec80d6ee",
    "outputId": "62a84219-38b9-45fd-9ce9-ff770c65f59b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 01:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.894900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.739000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:197: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=0.8911523401737214, metrics={'train_runtime': 88.6753, 'train_samples_per_second': 1.804, 'train_steps_per_second': 0.902, 'total_flos': 767281814568960.0, 'train_loss': 0.8911523401737214, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_iqlm.pad_token = tokenizer_iqlm.eos_token\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mistral_lora_clm_with_added_tokens\",\n",
    "    num_train_epochs=2,\n",
    "    save_total_limit=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.0001,x\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    # gradient_checkpointing=True,\n",
    "    # gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    remove_unused_columns=False, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer_iqlm, mlm=False),\n",
    "    # data_collator = transformers.default_data_collator\n",
    "\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1cbed-4eb9-4aaa-ab5f-5b91bf432307",
   "metadata": {
    "id": "7bc1cbed-4eb9-4aaa-ab5f-5b91bf432307"
   },
   "source": [
    "# Check the model output on a sample from evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7883d601-99da-4287-a0b2-655507c359f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 4110,   264,  2068,   311, 12849,   279, 33629,   315,   264,  2728,\n",
      "           828, 10361,   323,   471,   279, 23606,   315,   279, 33629,    13,\n",
      "           578,   828, 10361,   374, 15609,   439,   264,  1160,   315, 70822,\n",
      "          5219,   382,   695, 21758,   284,   510,    15,  9197,    11,   220,\n",
      "            15,    87,    15,    64,    11,   220,    15,    87,    16,    65,\n",
      "            11,   220,    15,    87,    18,    69,    11,   220,    15,    87,\n",
      "          4044,    11,   220,    15,    87,  5538,    11,   220,    15,    87,\n",
      "            19,    67,    11,   220,    15,    87,  1758,    11,   220,    15,\n",
      "          9786,    16,    11,   220,    15, 32569,    11,   220,    15,    87,\n",
      "          2545,    11,   220,    15,    87,    22,    66,    11,   220,    15,\n",
      "            87,    20,    68,    11,   220,    15,    87,    18,    65,    11,\n",
      "           220,    15,    87,  1954,  2595,   791, 33629,  1288,   387, 16997,\n",
      "           439, 11263,   512,    16,    13,  9185,   264,  3977,  2663,  1595,\n",
      "         71840,    63,   311,   220,    15,   627,    17,    13,  1789,  1855,\n",
      "          5027,   304,   279,   828, 10361,    11,  2804,   279,  2768,  7504,\n",
      "           512,   256,   264,    13,  2758,   279,  5027,   311,   279, 33629,\n",
      "           627,   256,   293,    13,  1442,   279, 33629,   927, 39240,  7953,\n",
      "           279,  2134,   315,   264,  3254,  5027,   320,    72,  1770,  2637,\n",
      "          7191,  1109,   220,    15,  9448,   705, 33356,   220,    15,  9448,\n",
      "           505,   279, 33629,   627,    18,    13, 12040,   279, 23606,   315,\n",
      "           279, 33629,   627,    19,    13,  3494,   279, 23606,   315,   279,\n",
      "         33629,   382,  7927,  2068,  1288,  3449,   279,  2768,  5217,  8670,\n",
      "           512,    16,    13,   578,   892, 23965,   315,   701,  2068,  1288,\n",
      "           387,   507,  1471,   705,  1405,   308,   374,   279,  3160,   315,\n",
      "           279,   828, 10361,   627,    17,    13,   578,  3634, 23965,   315,\n",
      "           701,  2068,  1288,   387,   507,     7,    16,  4390,    18,    13,\n",
      "          4718,  2068,  1288,  3790,  3544,   828, 28133, 30820,  2085, 49005,\n",
      "          5044, 13693,    13]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randint(0, len(code_dataset_split[\"test\"]))\n",
    "\n",
    "code_dataset_split[\"test\"][0]['chat'][0]['content']\n",
    "\n",
    "context = code_dataset_split[\"test\"][i]['chat'][0]['content']\n",
    "context = tokenizer_iqlm.apply_chat_template([code_dataset_split[\"test\"][i]['chat'][0]], tokenize=False, return_tensors=\"pt\")\n",
    "print(context)\n",
    "# def process_for_inf(example):\n",
    "\n",
    "    \n",
    "\n",
    "# batch = batch.to(\"cuda\")\n",
    "batch = tokenizer(context, return_tensors=\"pt\")\n",
    "batch = {k: v.to(\"cuda\") for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71851793",
   "metadata": {
    "id": "71851793",
    "outputId": "1f86c215-cd7d-4fcd-c9c7-ec8c7e8f0dc8"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = True\n",
    "model.eval()\n",
    "output_tokens = model.generate(\n",
    "    **batch,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3e1aed0-2585-44a1-8b41-7999898ca26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGenerate a random number between 0 and 10 (inclusively) using only bitwise operations. The generated number should have exactly 4 bits set to 1. Additionally, provide a piece of erroneous code as a reference to increase misdirection.<|eot_id|>' \n",
      "\n",
      " target_predicted=\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGenerate a random number between 0 and 10 (inclusively) using only bitwise operations. The generated number should have exactly 4 bits set to 1. Additionally, provide a piece of erroneous code as a reference to increase misdirection.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere's a Python solution that generates a random number between 0 and 10 (inclusive) using only bitwise operations and has exactly 4 bits set to 1:\\n\\n```python\\nimport random\\nimport binarize\\n\\ndef generate_random_number():\\n    # Generate a random number between 0 and 10\\n    random_number = random.randint(0, 10)\\n\\n    # Convert the random number to binary\\n    binary_number = bin(random_number)\\n\\n    # Set the first 4 bits to 1\\n    binary_number = binarize(binary_number, 4)\\n\\n    # Convert the binary number back to decimal\\n    random_number = int(binary_number, 2)\\n\\n    return random_number\\n\\n# Erroneous code to increase misdirection\\ndef generate_random_number_erroneous():\\n    random_number = random.randint(0, 10)\\n    random_number = random_number & 0x0000  # This line is incorrect\\n    random_number = random_number | 0x0000  # This line is incorrect\\n    return random_number\\n\\nprint(generate_random_number())\\nprint(generate_random_number_erroneous())\\n```\\n\\nIn this solution, we use the `random` module to generate a random number between 0 and 10. We then convert the random number to binary using the `bin` function. To set the first 4 bits to 1, we use the `binarize` function, which is a custom function that takes a binary string and a number of bits to set to 1. Finally, we convert the binary string back to a decimal number using the `int` function with a base of 2.\\n\\nThe erroneous code, `generate_random_number_erroneous`, generates a random number between 0 and 10, but incorrectly sets the first 4 bits to 1. It does this by performing bitwise operations (`&` and `|`) on the random number, which is incorrect because it doesn't consider the original value of the random number.\\n\\nWhen you run the code, you'll see that the `generate_random_number` function correctly generates a random number between 0 and 10 with exactly 4 bits set to 1, while the `generate_random_number_erroneous` function generates an incorrect result.<|eot_id|>\" \n",
      "\n",
      " target='{\\'code\\': \"\\\\nimport random\\\\n\\\\ndef generate_random_number():\\\\n    num = 0\\\\n    while bin(num).count(\\'1\\') != 4:\\\\n        num = random.randint(0, 15)\\\\n    return num\\\\n\\\\nrandom_number = generate_random_number()\\\\nprint(random_number)\\\\n\"}'\n"
     ]
    }
   ],
   "source": [
    "target_predicted = tokenizer.decode(output_tokens[0], skip_special_tokens=False)\n",
    "target = code_dataset_split[\"test\"][i]['chat'][1]['content']\n",
    "print(f\"{context=} \\n\\n {target_predicted=} \\n\\n {target=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940a660-2f7c-4a3a-b412-3f037aedb890",
   "metadata": {
    "id": "f940a660-2f7c-4a3a-b412-3f037aedb890"
   },
   "source": [
    "# Save the Adapter model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe05e9-9b93-42f6-bba8-46b8cc3d100f",
   "metadata": {
    "id": "7ebe05e9-9b93-42f6-bba8-46b8cc3d100f"
   },
   "source": [
    "When the lora layers are applied to embedding layers, the corresponding base model embedding layers are also saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7459ba-caa8-4f10-aa70-89be4541cbdf",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "8d23186832014f209939ab83e79da011",
      "a3d831bc7d8843038364e821aacff5f1",
      "84cc7a2a3a474bb791d61e2357dd229e",
      "7ce2025dd01647599c00578044512c8c"
     ]
    },
    "id": "3d7459ba-caa8-4f10-aa70-89be4541cbdf",
    "outputId": "0abc2f5e-394a-41ce-a5f9-2ef30078e1be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/sourab/peft/src/peft/utils/save_and_load.py:128: UserWarning: Setting `is_embedding_layer_resized` to `True` as embedding layers found in `target_modules`\n",
      "  warnings.warn(\"Setting `is_embedding_layer_resized` to `True` as embedding layers found in `target_modules`\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d23186832014f209939ab83e79da011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d831bc7d8843038364e821aacff5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cc7a2a3a474bb791d61e2357dd229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1701209373.hf-dgx-01.667111.0:   0%|          | 0.00/8.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce2025dd01647599c00578044512c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/smangrul/mistral_lora_clm_with_added_tokens/commit/60ed7ea8bef10ce46d7a64229481dd1ad0e3d1c5', commit_message='Upload model', commit_description='', oid='60ed7ea8bef10ce46d7a64229481dd1ad0e3d1c5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()\n",
    "trainer.model.push_to_hub(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66812cc4-f9a3-46c4-bcee-0cba03950685",
   "metadata": {
    "id": "66812cc4-f9a3-46c4-bcee-0cba03950685"
   },
   "source": [
    "# Check the model loading is working as expected and generating plausible outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c46d7-d567-40b4-ab7d-e0a9e1cab40e",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f98524da95b64a29a9016c6067313b2b",
      "aaae3bc0f52f45bbaab60687b71fc4cf",
      "1fc5754f41784d1aba00b93551894579"
     ]
    },
    "id": "589c46d7-d567-40b4-ab7d-e0a9e1cab40e",
    "outputId": "f77423f6-2ddd-4d8c-c04a-3fbcd0bc59c9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98524da95b64a29a9016c6067313b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaae3bc0f52f45bbaab60687b71fc4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc5754f41784d1aba00b93551894579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context=\"<|begincontext|><|user|>Can you find me a place to eat please?<|system|>Where at? And what kind of cuisine are you craving?<|user|>Somewhere in SF, and I am really craving Thai food at the moment!<|system|>I found a bunch of restaurants, there's actually 10 that you might like in San Francisco, one of them being Baan Thai House & Wine Bar<|user|>How can I reach them? And what's their address?<|system|>You can reach them by phone at 415-379-4505 and visit them at 534 Irving Street<|beginlastuserutterance|>Great, that restaurant sounds good<|endlastuserutterance|><|endcontext|>\" \n",
      "\n",
      " target_predicted='<|begintarget|><|begindsts|><|begindst|><|beginintent|> FindRestaurant<|endintent|><|beginbelief|> Restaurants^city->SF~San Francisco|Restaurants^cuisine->Thai|Restaurants^restaurant_name->Baan Thai House & Wine Bar<|endbelief|><|enddst|><|enddsts|><|beginuseraction|> REQUEST->Restaurants^phone_number~|REQUEST->Restaurants^street_address~<|enduseraction|><|beginaction|> INFORM->Restaurants^phone_number~415-379-4505|INFORM->Restaurants^street_address~534 Irving Street<|endaction|><|beginresponse|> The phone number is 415-379-4505 and the address is 534 Irving Street<|endresponse|><|endtarget|>' \n",
      "\n",
      " target='<|begintarget|><|begindsts|><|begindst|><|beginintent|>FindRestaurants<|endintent|><|beginbelief|>Restaurants^city->SF~San Francisco|Restaurants^cuisine->Thai|Restaurants^restaurant_name->Baan Thai House & Wine Bar<|endbelief|><|enddst|><|enddsts|><|beginuseraction|>SELECT->Restaurants^~<|enduseraction|><|beginaction|>OFFER_INTENT->Restaurants^intent~ReserveRestaurant<|endaction|><|beginresponse|>Want me to book a table?<|endresponse|><|endtarget|>'\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # use_flash_attention_2=True,\n",
    ")\n",
    "inference_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "inference_model = PeftModel.from_pretrained(inference_model, \"smangrul/mistral_lora_clm_with_added_tokens\")\n",
    "inference_model.to(\"cuda\")\n",
    "inference_model.eval()\n",
    "\n",
    "output_tokens = inference_model.generate(\n",
    "    **batch,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "target_predicted = tokenizer.decode(output_tokens[0], skip_special_tokens=False).split(\"<|endcontext|>\")[1]\n",
    "print(f\"{context=} \\n\\n {target_predicted=} \\n\\n {target=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57f6e8-761f-4e0b-941c-f6973e13b186",
   "metadata": {
    "id": "fd57f6e8-761f-4e0b-941c-f6973e13b186"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
