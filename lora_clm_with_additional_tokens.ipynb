{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f239612-620e-4430-8685-9fdc6b179b41",
   "metadata": {
    "id": "5f239612-620e-4430-8685-9fdc6b179b41"
   },
   "source": [
    "# Training PEFT models with new tokens being added to the embedding layers and tokenizer\n",
    "\n",
    "In this example, we will learn how to train a LoRA model when adding new tokens to the tokenizer and model.\n",
    "This is a common usecase when doing the following:\n",
    "1. Instruction finetuning with new tokens beind added such as `<|user|>`, `<|assistant|>`, `<|system|>`, `</s>`, `<s>` to properly format the conversations\n",
    "2. Finetuning on a specific language wherein language spoecific tokens are added, e.g., korean tokens being added to vocabulary for finetuning LLM on Korean datasets.\n",
    "3. Instruction finetuning to return outputs in certain format to enable agent behaviour new tokens such as `<|FUNCTIONS|>`, `<|BROWSE|>`, `<|TEXT2IMAGE|>`, `<|ASR|>`, `<|TTS|>`, `<|GENERATECODE|>`, `<|RAG|>`.\n",
    "\n",
    "In such cases, you add the Embedding modules to the LORA `target_modules`. PEFT will take care of saving the embedding layers with the new added tokens along with the adapter weights that were trained on the specific initialization of the embeddings weights of the added tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072e7c9-c1a5-4d61-a8ea-c5751769c430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b27c55e8-edaa-4059-90bc-d6096d596902",
   "metadata": {
    "id": "b27c55e8-edaa-4059-90bc-d6096d596902"
   },
   "source": [
    "Let's import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d413556b-4a3c-4ede-8642-ea5e772d83a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install aqlm[gpu]>=1.1.0\n",
    "!pip install git+https://github.com/huggingface/peft.git@main\n",
    "!pip install accelerate>=0.27.0\n",
    "!pip install git+https://github.com/huggingface/transformers.git@main\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes # for 8-bit optimizer only\n",
    "!pip install torch --upgrade\n",
    "!pip install huggingface --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fe0d78b-d846-4055-82b7-1232a0b64ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataclass_csv in /usr/local/lib/python3.8/dist-packages (1.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: aqlm 1.1.6\n",
      "Uninstalling aqlm-1.1.6:\n",
      "  Successfully uninstalled aqlm-1.1.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting aqlm[gpu]\n",
      "  Using cached aqlm-1.1.6-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from aqlm[gpu]) (2.3.1+cu121)\n",
      "Requirement already satisfied: transformers>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from aqlm[gpu]) (4.42.0.dev0)\n",
      "Requirement already satisfied: accelerate>=0.27.0 in /usr/local/lib/python3.8/dist-packages (from aqlm[gpu]) (0.31.0)\n",
      "Requirement already satisfied: triton>=2.1 in /usr/local/lib/python3.8/dist-packages (from aqlm[gpu]) (2.3.1)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.8/dist-packages (from aqlm[gpu]) (1.11.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch>=2.2.0->aqlm[gpu]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.8/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->aqlm[gpu]) (12.5.40)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.38.0->aqlm[gpu]) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.38.0->aqlm[gpu]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.38.0->aqlm[gpu]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.38.0->aqlm[gpu]) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=2.2.0->aqlm[gpu]) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.38.0->aqlm[gpu]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.38.0->aqlm[gpu]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.38.0->aqlm[gpu]) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.38.0->aqlm[gpu]) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=2.2.0->aqlm[gpu]) (1.3.0)\n",
      "Using cached aqlm-1.1.6-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: aqlm\n",
      "Successfully installed aqlm-1.1.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install dataclass_csv\n",
    "!pip uninstall aqlm -y\n",
    "!pip install aqlm[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f864c90",
   "metadata": {
    "id": "6f864c90"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"PeftExamples\"\n",
    "import transformers\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    # prepare_model_for_int8_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from dataclass_csv import DataclassReader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74950a3f-bb63-4ce5-9e2b-1b83f92b13a2",
   "metadata": {
    "id": "74950a3f-bb63-4ce5-9e2b-1b83f92b13a2"
   },
   "source": [
    "## Prepare Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76763f5e-64b2-409b-8845-ae5589f8a4e0",
   "metadata": {
    "id": "76763f5e-64b2-409b-8845-ae5589f8a4e0"
   },
   "source": [
    "Now, we will be adding 27 new tokens as well as replace the existing pad, bos and eos tokens of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0498ea-547e-418d-bf13-c9abafdd5476",
   "metadata": {
    "id": "fd0498ea-547e-418d-bf13-c9abafdd5476"
   },
   "outputs": [],
   "source": [
    "class SpecialTokens(str, Enum):\n",
    "    begin_target = \"<|begintarget|>\"\n",
    "    end_target = \"<|endtarget|>\"\n",
    "    begin_context = \"<|begincontext|>\"\n",
    "    end_context = \"<|endcontext|>\"\n",
    "    system = \"<|system|>\"\n",
    "    user = \"<|user|>\"\n",
    "    begin_last_user_utterance = \"<|beginlastuserutterance|>\"\n",
    "    end_last_user_utterance = \"<|endlastuserutterance|>\"\n",
    "    begin_dsts = \"<|begindsts|>\"\n",
    "    end_dsts = \"<|enddsts|>\"\n",
    "    begin_dst = \"<|begindst|>\"\n",
    "    end_dst = \"<|enddst|>\"\n",
    "    begin_belief = \"<|beginbelief|>\"\n",
    "    end_belief = \"<|endbelief|>\"\n",
    "    begin_response = \"<|beginresponse|>\"\n",
    "    end_response = \"<|endresponse|>\"\n",
    "    begin_action = \"<|beginaction|>\"\n",
    "    end_action = \"<|endaction|>\"\n",
    "    begin_user_action = \"<|beginuseraction|>\"\n",
    "    end_user_action = \"<|enduseraction|>\"\n",
    "    sys_actions = \"<|sysactions|>\"\n",
    "    begin_intent = \"<|beginintent|>\"\n",
    "    end_intent = \"<|endintent|>\"\n",
    "    begin_requested_slots = \"<|beginrequestedslots|>\"\n",
    "    end_requested_slots = \"<|endrequestedslots|>\"\n",
    "    pad_token = \"<|pad|>\"\n",
    "    bos_token = \"<|startoftext|>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a4255-5f13-4eef-a024-4f1de0f2173b",
   "metadata": {
    "id": "ae4a4255-5f13-4eef-a024-4f1de0f2173b"
   },
   "source": [
    "We will be finetuning Mistral-7B model. Let's load the tokenizer and add the special tokens followed by loading the base model and resizzing the embedding layers to accomodate the newly added tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0eedef9",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "91c67b6377fc4dd7977bf544de784d51"
     ]
    },
    "id": "f0eedef9",
    "outputId": "6306ccce-1661-4b1b-baa8-635013200c6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128256, 4096)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name = \"ISTA-DASLab/Meta-Llama-3-8B-Instruct-AQLM-2Bit-1x16\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name,\n",
    "#     pad_token=SpecialTokens.pad_token.value,\n",
    "#     bos_token=SpecialTokens.bos_token.value,\n",
    "#     eos_token=SpecialTokens.end_target.value,\n",
    "#     additional_special_tokens=SpecialTokens.list(),\n",
    "#     token=\"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\"\n",
    "# )\n",
    "\n",
    "\n",
    "tokenizer_iqlm = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer_iqlm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88439ed6-9974-4918-80df-ec78b05b4185",
   "metadata": {
    "id": "88439ed6-9974-4918-80df-ec78b05b4185"
   },
   "source": [
    "## Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80967087",
   "metadata": {
    "id": "80967087",
    "outputId": "23df1152-5ada-4641-f4a9-73367a237208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 44,204,032 || all params: 2,086,375,424 || trainable%: 2.1187\n",
      "None\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PeftModel(\n",
      "      (base_model): LoraModel(\n",
      "        (model): LlamaForCausalLM(\n",
      "          (model): LlamaModel(\n",
      "            (embed_tokens): lora.Embedding(\n",
      "              (base_layer): Embedding(128256, 4096)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict()\n",
      "              (lora_B): ModuleDict()\n",
      "              (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 64x128256 (cuda:0)])\n",
      "              (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 4096x64 (cuda:0)])\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (layers): ModuleList(\n",
      "              (0-31): 32 x LlamaDecoderLayer(\n",
      "                (self_attn): LlamaSdpaAttention(\n",
      "                  (q_proj): lora.AqlmLoraLinear(\n",
      "                    (base_layer): QuantizedLinear()\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k_proj): QuantizedLinear()\n",
      "                  (v_proj): lora.AqlmLoraLinear(\n",
      "                    (base_layer): QuantizedLinear()\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o_proj): QuantizedLinear()\n",
      "                  (rotary_emb): LlamaRotaryEmbedding()\n",
      "                )\n",
      "                (mlp): LlamaMLP(\n",
      "                  (gate_proj): QuantizedLinear()\n",
      "                  (up_proj): QuantizedLinear()\n",
      "                  (down_proj): QuantizedLinear()\n",
      "                  (act_fn): SiLU()\n",
      "                )\n",
      "                (input_layernorm): LlamaRMSNorm()\n",
      "                (post_attention_layernorm): LlamaRMSNorm()\n",
      "              )\n",
      "            )\n",
      "            (norm): LlamaRMSNorm()\n",
      "          )\n",
      "          (lm_head): lora.Linear(\n",
      "            (base_layer): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=64, out_features=128256, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "            (lora_magnitude_vector): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=64, lora_alpha=128, lora_dropout=0.0, target_modules=[\"embed_tokens\", \"lm_head\", \"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print(model.print_trainable_parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac9945-4fcb-45f4-9478-d99a25a519cc",
   "metadata": {
    "id": "15ac9945-4fcb-45f4-9478-d99a25a519cc"
   },
   "source": [
    "## Preapre Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c6980d59-42d4-4a27-84cc-a9719302088b",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "33d9539232da48f3ae922216b98ae462",
      "b7a33811d93742099140240cad91b679"
     ]
    },
    "id": "c6980d59-42d4-4a27-84cc-a9719302088b",
    "outputId": "0a0bcd05-da9c-4a98-f1e8-13585feeea42"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa13cf1fb99047ef8a9d985a4ce59d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462bd6641c01422ea09ed2285d687da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"smangrul/assistant_chatbot_dataset\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.2)\n",
    "\n",
    "text_column = \"context\"\n",
    "label_column = \"target\"\n",
    "max_length = 512\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(examples[text_column])\n",
    "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = model_inputs[\"input_ids\"][i][:max_length]\n",
    "        model_inputs[\"attention_mask\"][i] = model_inputs[\"attention_mask\"][i][:max_length]\n",
    "        labels[\"input_ids\"][i] = labels[\"input_ids\"][i][:max_length]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f38888e-4382-415b-869d-7202a816606a",
   "metadata": {
    "id": "3f38888e-4382-415b-869d-7202a816606a"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuf8ufle=True, collate_fn=default_data_collator, batch_size=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b9e552-4c5d-43e8-a9cd-8073af8d4280",
   "metadata": {
    "id": "53b9e552-4c5d-43e8-a9cd-8073af8d4280",
    "outputId": "047172a2-bebe-4e12-8fd0-d223f3198599"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         ...,\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257],\n",
       "         [128258, 128258, 128258,  ..., 128273, 128257, 128257]]),\n",
       " 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         ...,\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257],\n",
       "         [  -100,   -100,   -100,  ..., 128273, 128257, 128257]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de31ee2-185e-4658-9ad1-ae5f6bc3a611",
   "metadata": {
    "id": "7de31ee2-185e-4658-9ad1-ae5f6bc3a611",
    "outputId": "7606e5bb-a86a-41cb-b896-eefb2c565163"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|begincontext|><|user|>Hey, can you help me find a place to eat?<|system|>Sure, where abouts?<|user|>I want to eat in SF.<|system|>What type of food do you fancy? Chinese, Indian?<|user|>I'm feeling some Szcheuan.<|system|>Okay, I've found 10 restaurants that could be good. How does Alice's in San Francisco sound?<|user|>Sounds good. What's their number and address?<|system|>Their number is 415-282-8999. The address is 1599 Sanchez Street.<|user|>That's great, thanks.<|system|>Would you like me to make a reservation for you?<|user|>Yes, please. On the 1st of this month.<|system|>And for what time?<|user|>For around 11:45.<|system|>Sure thing, please confirm: reservation for 2 people at Alice's in San Francisco at 11:45 am today.<|beginlastuserutterance|>Yes that's good. Are they expensive? Is there live music?<|endlastuserutterance|><|endcontext|><|begintarget|><|begindsts|><|begindst|><|beginintent|>ReserveRestaurant<|endintent|><|beginrequestedslots|>Restaurants^has_live_music|Restaurants^price_range<|endrequestedslots|><|beginbelief|>Restaurants^city->SF~San Francisco|Restaurants^cuisine->Chinese~Szcheuan|Restaurants^date->1st of this month~today|Restaurants^party_size->2|Restaurants^restaurant_name->Alice's|Restaurants^time->11:45~11:45 am<|endbelief|><|enddst|><|enddsts|><|beginuseraction|>REQUEST->Restaurants^price_range~|REQUEST->Restaurants^has_live_music~|AFFIRM->Restaurants^~<|enduseraction|><|beginaction|>INFORM->Restaurants^price_range~inexpensive|INFORM->Restaurants^has_live_music~False|OFFER->Restaurants^restaurant_name~Alice's|OFFER->Restaurants^party_size~2|OFFER->Restaurants^date~today|OFFER->Restaurants^time~12 pm|NOTIFY_FAILURE->Restaurants^~<|endaction|><|beginresponse|>I'm sorry, I was unable to make a reservation at that time. Shall I try for a reservation at Alice's for 2 people today at 12 pm? Their pricing is inexpensive and they do not have live music.<|endresponse|><|endtarget|><|endtarget|>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0358e3-0e22-4a91-be62-c396b04cf332",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031a7ca4-c157-4de8-b559-e04e0012a75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Create a nested loop to print every combination of numbers between 0-9, excluding any combination that contains the number 5. Additionally, exclude any combination that contains a repeating digit. Implement the solution without using any built-in functions or libraries to check for repeating digits.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"{'code': '\\\\nfor i in range(10):  # First digit\\\\n    for j in range(10):  # Second digit\\\\n        for k in range(10):  # Third digit\\\\n            # Checking for the conditions\\\\n            if i != 5 and j != 5 and k != 5 and i != j and i != k and j != k:\\\\n                print(i, j, k)\\\\n'}\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset(\"m-a-p/CodeFeedback-Filtered-Instruction\")\n",
    "\n",
    "def get_code(s):\n",
    "    s = s.split(\"```\")\n",
    "    for p in s:\n",
    "        if p.startswith('python'):\n",
    "            return p[6:]\n",
    "\n",
    "new_data = []\n",
    "for x in raw_data['train']:\n",
    "\n",
    "    curr_chat = []\n",
    "    code = get_code(x['answer'])\n",
    "    if code is None or code==\"None\":\n",
    "        continue\n",
    "\n",
    "    chat = [\n",
    "    {\"role\": \"user\", \"content\": x[\"query\"]},\n",
    "    {\"role\": \"assistant\", \"content\": str({\"code\": code})}]\n",
    "    \n",
    "    new_data.append(chat)\n",
    "\n",
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43910e8d-0f85-4e80-b009-50ae8c05540a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a4191d16084c71bb1b3de4326dd92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "tokenizer_iqlm.pad_token = tokenizer_iqlm.eos_token\n",
    "train_dataset2 = Dataset.from_dict({\"chat\": new_data})\n",
    "\n",
    "\n",
    "train_dataset2 = train_dataset2.map(lambda x: {\"input_ids\": tokenizer_iqlm.apply_chat_template(x[\"chat\"], tokenize=True,  \n",
    "                                    truncation=True, padding=True, max_length=512)}, \n",
    "                                    batched=True)\n",
    "\n",
    "\n",
    "# def tokenize_dataset(data):\n",
    "#     # Keys of the returned dictionary will be added to the dataset as columns\n",
    "#     return tokenizer(data[\"formatted_chat\"])\n",
    "\n",
    "# train_dataset = dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2977bb88-2aac-4ceb-bb09-7313158ad445",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset2 = train_dataset2.remove_columns([col for col in train_dataset2.column_names if col != \"input_ids\"])\n",
    "train_dataset2.set_format(\"torch\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b987694-a9c9-4a26-88c7-e899ec8eec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader2 = DataLoader(\n",
    "    train_dataset2, shuffle=True, collate_fn=default_data_collator, batch_size=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7ec081a-2848-464c-942b-5a8c0cb598a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWrite a function to find the number of distinct states in a given matrix. Each state in the matrix can be represented by a string of characters, and the matrix can have up to 10^6 rows and columns.\\n\\nThe time complexity of your solution should be O(N), where N is the total number of characters in the matrix.\\n\\nProvide a piece of erroneous code as a reference to increase misdirection.\\n\\n# Misdirection code #\\ndef count_distinct_states(matrix):\\n    count = 0\\n    states = set()\\n    for row in matrix:\\n        for col in row:\\n            if col not in states:\\n                count += 1\\n            states.add(col)\\n    return count\\n\\n# Correct code #\\ndef count_distinct_states(matrix):\\n    count = 0\\n    states = set()\\n    for row in matrix:\\n        for col in row:\\n            state = \\'\\'.join(col)\\n            if state not in states:\\n                count += 1\\n            states.add(state)\\n    return count\\n\\nmatrix = [[\\'A\\', \\'B\\', \\'C\\'],\\n          [\\'A\\', \\'B\\', \\'D\\'],\\n          [\\'A\\', \\'B\\', \\'C\\']]\\nprint(count_distinct_states(matrix))\\n# Output: 4<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{\\'code\\': \"\\\\ndef count_distinct_states(matrix):\\\\n    count = 0\\\\n    states = set()\\\\n    for row in matrix:\\\\n        for col in row:\\\\n            state = \\'\\'.join(col)\\\\n            if state not in states:\\\\n                count += 1\\\\n            states.add(state)\\\\n    return count\\\\n\\\\nmatrix = [[\\'A\\', \\'B\\', \\'C\\'],\\\\n          [\\'A\\', \\'B\\', \\'D\\'],\\\\n          [\\'A\\', \\'B\\', \\'C\\']]\\\\nprint(count_distinct_states(matrix))\\\\n# Output: 4\\\\n\"}<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset2[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d1c83-196d-471e-9bf7-5f36dafa9894",
   "metadata": {
    "id": "239d1c83-196d-471e-9bf7-5f36dafa9894"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80d6ee",
   "metadata": {
    "id": "ec80d6ee",
    "outputId": "62a84219-38b9-45fd-9ce9-ff770c65f59b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "CODECARBON : Failed to match CPU TDP constant. Falling back on a global constant.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='20030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  250/20030 07:02 < 9:21:34, 0.59 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.856700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.740900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.700200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.710500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.575800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mistral_lora_clm_with_added_tokens\",\n",
    "    num_train_epochs=2,\n",
    "    save_total_limit=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.0001,\n",
    "    dataloader_drop_last=True,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    remove_unused_columns=False, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset2,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1cbed-4eb9-4aaa-ab5f-5b91bf432307",
   "metadata": {
    "id": "7bc1cbed-4eb9-4aaa-ab5f-5b91bf432307"
   },
   "source": [
    "# Check the model output on a sample from evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71851793",
   "metadata": {
    "id": "71851793",
    "outputId": "1f86c215-cd7d-4fcd-c9c7-ec8c7e8f0dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context=\"<|begincontext|><|user|>I'd like to find a place to eat in this city.<|system|>What kind of restaurants are you interested in? Which city should I search?<|user|>Look for burger restaurants in Campbell.<|system|>There's a good restaurant in Campbell called Effie's Restaurant & Bar<|user|>What is Effie's address and do they serve drinks?<|system|>They don't serve drinks but their address is 331 West Hacienda Avenue.<|user|>I'd like to make a reservation there.<|system|>What time would you like your reservation?<|user|>Make it on the 4th of this month please in the evening 6:30.<|system|>Please confirm your request: You'd like to book a table for 2 at Effie's Restaurant & Bar in Campbell at 6:30 pm for next Monday.<|user|>No I'd like a reservation on the 1st.<|system|>Please confirm the city you're booking in is Campbell and the date is for today.<|user|>No the reservation is for 3 people.<|system|>Please confirm: the reservation you want is for 6:30 pm for 3 people.<|user|>Yes, sounds good.<|system|>Great, I made the reservation.<|beginlastuserutterance|>Do they play live music and what is their phone number?<|endlastuserutterance|><|endcontext|>\" \n",
      "\n",
      " target_predicted=\"<|begintarget|><|begindsts|><|begindst|><|beginintent|>ReserveRestaurant<|endintent|><|beginrequestedslots|>Restaurants^phone_number|Restaurants^has_live_music<|endrequestedslots|><|beginbelief|>Restaurants^city->Campbell|Restaurants^cuisine->burger|Restaurants^date->today|Restaurants^party_size->3|Restaurants^restaurant_name->Effie's Restaurant & Bar|Restaurants^time->6:30 pm~6:30 pm<|endbelief|><|enddst|><|enddsts|><|beginuseraction|>REQUEST->Restaurants^phone_number~|REQUEST->Restaurants^has_live_music~|AFFIRM->Restaurants^~<|enduseraction|><|beginaction|>INFORM->Restaurants^phone_number~510-555-5555|INFORM->Restaurants^has_live_music~False|NOTIFY_SUCCESS->Restaurants^~<|endaction|><|beginresponse|>The reservation is successful. The phone number is 510-555-5555. They don't have live music.<|endresponse|><|endtarget|>\" \n",
      "\n",
      " target=\"<|begintarget|><|begindsts|><|begindst|><|beginintent|>ReserveRestaurant<|endintent|><|beginrequestedslots|>Restaurants^has_live_music|Restaurants^phone_number<|endrequestedslots|><|beginbelief|>Restaurants^city->Campbell|Restaurants^cuisine->burger|Restaurants^date->the 1st~today|Restaurants^party_size->3|Restaurants^restaurant_name->Effie's Restaurant & Bar|Restaurants^time->6:30 pm~evening 6:30<|endbelief|><|enddst|><|enddsts|><|beginuseraction|>REQUEST->Restaurants^has_live_music~|REQUEST->Restaurants^phone_number~<|enduseraction|><|beginaction|>INFORM->Restaurants^phone_number~408-374-3400|INFORM->Restaurants^has_live_music~False<|endaction|><|beginresponse|>Their phone number is 408-374-3400 and they don't have live music.<|endresponse|><|endtarget|>\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randint(0, len(dataset[\"test\"]))\n",
    "context = dataset[\"test\"][i][\"context\"]\n",
    "\n",
    "batch = tokenizer(context, return_tensors=\"pt\")\n",
    "batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "model.eval()\n",
    "output_tokens = model.generate(\n",
    "    **batch,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "target_predicted = tokenizer.decode(output_tokens[0], skip_special_tokens=False).split(\"<|endcontext|>\")[1]\n",
    "target = dataset[\"test\"][i][\"target\"]\n",
    "print(f\"{context=} \\n\\n {target_predicted=} \\n\\n {target=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940a660-2f7c-4a3a-b412-3f037aedb890",
   "metadata": {
    "id": "f940a660-2f7c-4a3a-b412-3f037aedb890"
   },
   "source": [
    "# Save the Adapter model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe05e9-9b93-42f6-bba8-46b8cc3d100f",
   "metadata": {
    "id": "7ebe05e9-9b93-42f6-bba8-46b8cc3d100f"
   },
   "source": [
    "When the lora layers are applied to embedding layers, the corresponding base model embedding layers are also saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7459ba-caa8-4f10-aa70-89be4541cbdf",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "8d23186832014f209939ab83e79da011",
      "a3d831bc7d8843038364e821aacff5f1",
      "84cc7a2a3a474bb791d61e2357dd229e",
      "7ce2025dd01647599c00578044512c8c"
     ]
    },
    "id": "3d7459ba-caa8-4f10-aa70-89be4541cbdf",
    "outputId": "0abc2f5e-394a-41ce-a5f9-2ef30078e1be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/sourab/peft/src/peft/utils/save_and_load.py:128: UserWarning: Setting `is_embedding_layer_resized` to `True` as embedding layers found in `target_modules`\n",
      "  warnings.warn(\"Setting `is_embedding_layer_resized` to `True` as embedding layers found in `target_modules`\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d23186832014f209939ab83e79da011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d831bc7d8843038364e821aacff5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cc7a2a3a474bb791d61e2357dd229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1701209373.hf-dgx-01.667111.0:   0%|          | 0.00/8.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce2025dd01647599c00578044512c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/smangrul/mistral_lora_clm_with_added_tokens/commit/60ed7ea8bef10ce46d7a64229481dd1ad0e3d1c5', commit_message='Upload model', commit_description='', oid='60ed7ea8bef10ce46d7a64229481dd1ad0e3d1c5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()\n",
    "trainer.model.push_to_hub(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66812cc4-f9a3-46c4-bcee-0cba03950685",
   "metadata": {
    "id": "66812cc4-f9a3-46c4-bcee-0cba03950685"
   },
   "source": [
    "# Check the model loading is working as expected and generating plausible outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c46d7-d567-40b4-ab7d-e0a9e1cab40e",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f98524da95b64a29a9016c6067313b2b",
      "aaae3bc0f52f45bbaab60687b71fc4cf",
      "1fc5754f41784d1aba00b93551894579"
     ]
    },
    "id": "589c46d7-d567-40b4-ab7d-e0a9e1cab40e",
    "outputId": "f77423f6-2ddd-4d8c-c04a-3fbcd0bc59c9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98524da95b64a29a9016c6067313b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaae3bc0f52f45bbaab60687b71fc4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc5754f41784d1aba00b93551894579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context=\"<|begincontext|><|user|>Can you find me a place to eat please?<|system|>Where at? And what kind of cuisine are you craving?<|user|>Somewhere in SF, and I am really craving Thai food at the moment!<|system|>I found a bunch of restaurants, there's actually 10 that you might like in San Francisco, one of them being Baan Thai House & Wine Bar<|user|>How can I reach them? And what's their address?<|system|>You can reach them by phone at 415-379-4505 and visit them at 534 Irving Street<|beginlastuserutterance|>Great, that restaurant sounds good<|endlastuserutterance|><|endcontext|>\" \n",
      "\n",
      " target_predicted='<|begintarget|><|begindsts|><|begindst|><|beginintent|> FindRestaurant<|endintent|><|beginbelief|> Restaurants^city->SF~San Francisco|Restaurants^cuisine->Thai|Restaurants^restaurant_name->Baan Thai House & Wine Bar<|endbelief|><|enddst|><|enddsts|><|beginuseraction|> REQUEST->Restaurants^phone_number~|REQUEST->Restaurants^street_address~<|enduseraction|><|beginaction|> INFORM->Restaurants^phone_number~415-379-4505|INFORM->Restaurants^street_address~534 Irving Street<|endaction|><|beginresponse|> The phone number is 415-379-4505 and the address is 534 Irving Street<|endresponse|><|endtarget|>' \n",
      "\n",
      " target='<|begintarget|><|begindsts|><|begindst|><|beginintent|>FindRestaurants<|endintent|><|beginbelief|>Restaurants^city->SF~San Francisco|Restaurants^cuisine->Thai|Restaurants^restaurant_name->Baan Thai House & Wine Bar<|endbelief|><|enddst|><|enddsts|><|beginuseraction|>SELECT->Restaurants^~<|enduseraction|><|beginaction|>OFFER_INTENT->Restaurants^intent~ReserveRestaurant<|endaction|><|beginresponse|>Want me to book a table?<|endresponse|><|endtarget|>'\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # use_flash_attention_2=True,\n",
    ")\n",
    "inference_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "inference_model = PeftModel.from_pretrained(inference_model, \"smangrul/mistral_lora_clm_with_added_tokens\")\n",
    "inference_model.to(\"cuda\")\n",
    "inference_model.eval()\n",
    "\n",
    "output_tokens = inference_model.generate(\n",
    "    **batch,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "target_predicted = tokenizer.decode(output_tokens[0], skip_special_tokens=False).split(\"<|endcontext|>\")[1]\n",
    "print(f\"{context=} \\n\\n {target_predicted=} \\n\\n {target=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57f6e8-761f-4e0b-941c-f6973e13b186",
   "metadata": {
    "id": "fd57f6e8-761f-4e0b-941c-f6973e13b186"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
